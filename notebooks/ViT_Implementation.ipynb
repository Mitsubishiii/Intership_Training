{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wcyrKcyHLSXN",
        "outputId": "71c86f69-3b64-4093-af05-76c19bfa2533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.0+cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hLPdRz2yKjbn",
        "outputId": "27363957-ad66-42d2-ac40-cfef1bd5ea7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "sI67RrUrW4km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COLOR_CHANNELS = 3\n",
        "IMG_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "EMBEDDING_DIM = 768\n",
        "BATCH_SIZE = 12\n",
        "NUM_HEADS = 12\n",
        "MLP_SIZE = 3072\n",
        "TRANSFORMER_LAYER_NUM = 12\n",
        "EMBEDDING_DROPOUT = 0.1\n",
        "CLASSES_NUM=1000\n",
        "PATCH_NUMBER = ( IMG_SIZE * IMG_SIZE ) // PATCH_SIZE ** 2"
      ],
      "metadata": {
        "id": "mmYK7nOSMBov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Patches number: {PATCH_NUMBER}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTTQXvlyP-C8",
        "outputId": "ff3edfba-2eb3-4ab1-b593-7e34b096ce40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patches number: 196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  \"\"\"\n",
        "  Turns a 2D input image into a 1D set of embedded patches.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               in_channels=COLOR_CHANNELS,\n",
        "               patch_size=PATCH_SIZE,\n",
        "               embedding_dim=EMBEDDING_DIM):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "      - in_channels = Number of color channel for the input image. Default 3.\n",
        "      - patch_size = Size of the patches to convert input image into. Default 16.\n",
        "      - embedding_dim = Size of the embedding vector to turn image into. Default 768.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "    self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                             out_channels=embedding_dim,\n",
        "                             kernel_size=patch_size,\n",
        "                             stride=patch_size,\n",
        "                             padding=0) # No padding here\n",
        "\n",
        "    self.flatter = nn.Flatten(start_dim=2, end_dim=3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Prior size verification\n",
        "    img_res = x.shape[-1]\n",
        "    assert img_res % self.patch_size == 0, \"Image resolution must be divisible by the patch size\"\n",
        "\n",
        "    x_patches = self.patcher(x)\n",
        "    x_flattened = self.flatter(x_patches)\n",
        "    x_embedded = x_flattened.permute(0, 2, 1)\n",
        "    return x_embedded"
      ],
      "metadata": {
        "id": "dDlt0i-9NoTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test tensor (B, C, H, W)\n",
        "input_tensor = torch.randn(BATCH_SIZE, COLOR_CHANNELS, IMG_SIZE, IMG_SIZE)\n",
        "print(f\"Input tensor shape : {input_tensor.shape}\")\n",
        "\n",
        "# Test PatchEmbedding class\n",
        "Embedder = PatchEmbedding()\n",
        "output_tensor = Embedder(input_tensor)\n",
        "print(f\"Output tensor shape : {output_tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhRkMJ0tU_8b",
        "outputId": "12f9c1fd-6ee2-42f8-cf6b-48cea937cd7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape : torch.Size([12, 3, 224, 224])\n",
            "Output tensor shape : torch.Size([12, 196, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Implements the multi head self attention block of the trasformer encoder.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "                embedding_dim=EMBEDDING_DIM,\n",
        "                num_heads=NUM_HEADS,\n",
        "                attn_dropout:float=0):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "      -embedding_dim: The constant latent vector size D used throughout the Transformer.\n",
        "      -num_heads: Number of attention heads (k).\n",
        "      -attn_dropout: Dropout probability applied to the attention weights.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.normalizer = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                 num_heads=num_heads,\n",
        "                                                 dropout=attn_dropout,\n",
        "                                                 batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.normalizer(x)\n",
        "    attn_output, _ = self.multihead_attn(query=x,\n",
        "                                         key=x,\n",
        "                                         value=x)\n",
        "    return attn_output"
      ],
      "metadata": {
        "id": "t1qeHSaOWN_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Implements the MLP block of the transformer encoder.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               embedding_dim=EMBEDDING_DIM,\n",
        "               mlp_size=MLP_SIZE,\n",
        "               mlp_dropout:float=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.normalizer = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=embedding_dim, out_features=mlp_size),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p=mlp_dropout),\n",
        "        nn.Linear(in_features=mlp_size, out_features=embedding_dim),\n",
        "        nn.Dropout(p=mlp_dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.normalizer(x)\n",
        "    x = self.mlp(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "CZuaTvS6qzwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  \"\"\"\n",
        "  Create Transformer encoder block.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               embeding_dim=EMBEDDING_DIM,\n",
        "               num_heads=NUM_HEADS,\n",
        "               mlp_size=MLP_SIZE,\n",
        "               attn_dropout:float=0,\n",
        "               mlp_dropout:float=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.msa_block = MultiHeadAttentionBlock(embedding_dim=embedding_dim,\n",
        "                              num_heads=num_heads)\n",
        "\n",
        "    self.mlp_block = MLPBlock(embedding_dim=embedding_dim,\n",
        "                              mlp_size=mlp_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.msa_block(x) + x\n",
        "    x = self.mlp_block(x) + x\n",
        "    return x"
      ],
      "metadata": {
        "id": "JnDHLDSMt1nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  \"\"\"\n",
        "  Create Vision Transformer architecture model.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               img_size=IMG_SIZE, # Training resolution\n",
        "               in_channels=COLOR_CHANNELS, # Number of color channels in input image\n",
        "               patch_size=PATCH_SIZE, # Patch size\n",
        "               transformer_layer_num=TRANSFORMER_LAYER_NUM, # Number of ViT layers from ViT paper table\n",
        "               embedding_dim=EMBEDDING_DIM, # Hidden D size from ViT paper table\n",
        "               mlp_size=MLP_SIZE, # MLP size from ViT paper table\n",
        "               num_heads=NUM_HEADS, # Number of heads for MSA from ViT paper table\n",
        "               attn_dropout:float=0, # Dropout for attention from ViT paper table\n",
        "               mlp_dropout:float=0, # Dropout for MLP layers from ViT paper table\n",
        "               embedding_dropout=EMBEDDING_DROPOUT, # Dropout for patch and positional embedding\n",
        "               num_classes=CLASSES_NUM): # Number of classes to predict\n",
        "    super().__init__()\n",
        "\n",
        "    # Make sure the image size is divisible by the patch size\n",
        "    assert img_size % patch_size == 0, \"Image resolution must be divisible by the patch size\"\n",
        "\n",
        "    # Number of patches\n",
        "    self.num_patches = (img_size * img_size) // patch_size ** 2\n",
        "\n",
        "    # Create learnable class embedding\n",
        "    self.class_embedding = nn.Paramenter(torch.randn(1, 1, embedding_dim))\n",
        "\n",
        "    # Create learnable positional embedding\n",
        "    self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embedding_dim))\n",
        "\n",
        "    # Dropout value for patch and positional embedding\n",
        "    self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "\n",
        "    # Create patch embedding layer\n",
        "    self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                          patch_size=patch_size,\n",
        "                                          embedding_dim=embedding_dim)\n",
        "\n",
        "    # Create Transformer blocks\n",
        "    self.transformer_layer = nn.Sequential(*[TransformerEncoder(embedding_dim=embedding_dim,\n",
        "                                                                num_heads=num_heads,\n",
        "                                                                mlp_size=mlp_size,\n",
        "                                                                attn_dropout=attn_dropout,\n",
        "                                                                mlp_dropout=mlp_dropout) for _ in range(transformer_layer_num)])\n",
        "\n",
        "    # Create classifier head\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "        nn.Linear(in_features=embedding_dim, out_features=num_classes))\n",
        "\n",
        "  def forward(self, x):\n",
        "     # Get batch size\n",
        "     batch_size = x.shape[0],\n",
        "\n",
        "     # Create class token embeddding and expand it to the batch size\n",
        "     class_token = self.class_embedding.expand(batch_size, -1, -1)\n",
        "\n",
        "     # Apply patch embedding\n",
        "     x = self.patch_embedding(x)\n",
        "\n",
        "     # Concatenate class embedding and patch embedding\n",
        "     x = torch.cat((class_token, x), dim=1)\n",
        "\n",
        "     # Add positional embedding\n",
        "     x = x + self.positional_embedding\n",
        "\n",
        "     # Apply dropout to embedding part\n",
        "     x = self.embedding_dropout(x)\n",
        "\n",
        "     # Pass patch, class and positional embedding through the tranformer blocks\n",
        "     x = self.transformer_layer(x)\n",
        "\n",
        "     # 0 logit for classifier\n",
        "     x = self.classifier(x[:, 0])\n",
        "\n",
        "     return x"
      ],
      "metadata": {
        "id": "gtKn4pRBqgQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}