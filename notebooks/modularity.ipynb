{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0175251",
   "metadata": {},
   "source": [
    "### Project insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c62b4f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory : c:\\Users\\Sacha\\Documents\\vision_project\n",
      "What's inside : ['data', 'models', 'modularity.ipynb', 'notebooks', 'src', 'train.py', 'train_CNN.py', 'venv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Working directory : {os.getcwd()}\")\n",
    "print(f\"What's inside : {os.listdir('.')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd99b2",
   "metadata": {},
   "source": [
    "### Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e8a26d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory exists.\n",
      "Downloading pizza, steak, sushi data...\n",
      "Unzipping pizza, steak, sushi data...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"data/\")\n",
    "\n",
    "# If the image folder doesn't exist, download it and prepare it... \n",
    "if data_path.is_dir():\n",
    "    print(f\"{data_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {data_path} directory, creating one...\")\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download pizza, steak, sushi data\n",
    "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "    print(\"Downloading pizza, steak, sushi data...\")\n",
    "    f.write(request.content)\n",
    "\n",
    "# Unzip pizza, steak, sushi data\n",
    "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "    print(\"Unzipping pizza, steak, sushi data...\") \n",
    "    zip_ref.extractall(data_path)\n",
    "\n",
    "# Remove zip file\n",
    "os.remove(data_path / \"pizza_steak_sushi.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f34860",
   "metadata": {},
   "source": [
    "#### Making the DataLoader generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "befc5665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/data_setup.py\n",
    "\"\"\"\n",
    "File to create PyTorch DataLoaders \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(train_dir: str, \n",
    "                       test_dir: str,\n",
    "                       transform: transforms.Compose,\n",
    "                       batch_size: int,\n",
    "                       num_workers: int=NUM_WORKERS,\n",
    "                       dataset_type: str = None)->DataLoader:\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Create for each dataset a set of iterables batches\n",
    "    \n",
    "    Arguments:\n",
    "    #  - train_dir: Path to the trai data directory\n",
    "    #  - test_dir: Path to the test data directory\n",
    "     - transform: Torchvision transforms to perform on data\n",
    "     - batch_size: Size of each batch (How many images per batch) in DataLoader\n",
    "     - num_workers: Number of workers per DataLoader. Default all threads available.\n",
    "\n",
    "    Returns:\n",
    "     - A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "     Where class_names is a list of the target classes.\n",
    "    \"\"\"\n",
    "    if dataset_type == \"cifar10\":\n",
    "        train_data = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
    "        test_data = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
    "    else:\n",
    "        # Ton code actuel pour les dossiers locaux\n",
    "        train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "        test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    # Turn each dataset into a set of iterables batches (DataLoaders)\n",
    "    train_dataloader = DataLoader(dataset=train_data,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=num_workers)\n",
    "\n",
    "    test_dataloader = DataLoader(dataset=test_data,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "    \n",
    "    return (train_dataloader, test_dataloader, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b3c53",
   "metadata": {},
   "source": [
    "#### Making the CNN model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3638591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/CNN_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/CNN_builder.py\n",
    "\"\"\" \n",
    "File for Pytorch code of the CNN model to instantiate.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# CNN Model\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture from https://poloclub.github.io/cnn-explainer/.\n",
    "    TinyVGG adaptation.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int):\n",
    "        \"\"\"\n",
    "        Initializes the CNN model layers.\n",
    "\n",
    "        Arguments:\n",
    "          - input_shape: Number of input color channels.\n",
    "          - hidden_units: Number of hidden units (filters) per convolutional layer.\n",
    "          - output_shape: Number of output units/classes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Assuming input images are 32x32. After two MaxPool2d (size/2), \n",
    "            # the feature map size is 8x8.\n",
    "            nn.Linear(in_features=hidden_units*8*8,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590166d1",
   "metadata": {},
   "source": [
    "### Making the ViT model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07d6f861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ViT_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ViT_builder.py\n",
    "\"\"\" \n",
    "File for Pytorch code of the ViT model to instantiate.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "COLOR_CHANNELS = 3\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 768\n",
    "BATCH_SIZE = 12\n",
    "NUM_HEADS = 12\n",
    "MLP_SIZE = 3072\n",
    "TRANSFORMER_LAYER_NUM = 12\n",
    "EMBEDDING_DROPOUT = 0.1\n",
    "CLASSES_NUM=1000\n",
    "PATCH_NUMBER = ( IMG_SIZE * IMG_SIZE ) // PATCH_SIZE ** 2\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "  \"\"\"\n",
    "  Turns a 2D input image into a 1D set of embedded patches.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               in_channels=COLOR_CHANNELS,\n",
    "               patch_size=PATCH_SIZE,\n",
    "               embedding_dim=EMBEDDING_DIM):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      - in_channels = Number of color channel for the input image. Default 3.\n",
    "      - patch_size = Size of the patches to convert input image into. Default 16.\n",
    "      - embedding_dim = Size of the embedding vector to turn image into. Default 768.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.patch_size = patch_size\n",
    "\n",
    "    self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                             out_channels=embedding_dim,\n",
    "                             kernel_size=patch_size,\n",
    "                             stride=patch_size,\n",
    "                             padding=0) # No padding here\n",
    "\n",
    "    self.flatter = nn.Flatten(start_dim=2, end_dim=3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Prior size verification\n",
    "    img_res = x.shape[-1]\n",
    "    assert img_res % self.patch_size == 0, \"Image resolution must be divisible by the patch size\"\n",
    "\n",
    "    x_patches = self.patcher(x)\n",
    "    x_flattened = self.flatter(x_patches)\n",
    "    x_embedded = x_flattened.permute(0, 2, 1)\n",
    "    return x_embedded\n",
    "  \n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "  \"\"\"\n",
    "  Implements the multi head self attention block of the trasformer encoder.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "                embedding_dim=EMBEDDING_DIM,\n",
    "                num_heads=NUM_HEADS,\n",
    "                attn_dropout:float=0):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      -embedding_dim: The constant latent vector size D used throughout the Transformer.\n",
    "      -num_heads: Number of attention heads (k).\n",
    "      -attn_dropout: Dropout probability applied to the attention weights.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.normalizer = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "    self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                 num_heads=num_heads,\n",
    "                                                 dropout=attn_dropout,\n",
    "                                                 batch_first=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.normalizer(x)\n",
    "    attn_output, _ = self.multihead_attn(query=x,\n",
    "                                         key=x,\n",
    "                                         value=x)\n",
    "    return attn_output\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "  \"\"\"\n",
    "  Implements the MLP block of the transformer encoder.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               embedding_dim=EMBEDDING_DIM,\n",
    "               mlp_size=MLP_SIZE,\n",
    "               mlp_dropout:float=0):\n",
    "    super().__init__()\n",
    "\n",
    "    self.normalizer = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(in_features=embedding_dim, out_features=mlp_size),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(p=mlp_dropout),\n",
    "        nn.Linear(in_features=mlp_size, out_features=embedding_dim),\n",
    "        nn.Dropout(p=mlp_dropout))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.normalizer(x)\n",
    "    x = self.mlp(x)\n",
    "    return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "  \"\"\"\n",
    "  Create Transformer encoder block.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               embedding_dim=EMBEDDING_DIM,\n",
    "               num_heads=NUM_HEADS,\n",
    "               mlp_size=MLP_SIZE,\n",
    "               attn_dropout:float=0,\n",
    "               mlp_dropout:float=0):\n",
    "    super().__init__()\n",
    "\n",
    "    self.msa_block = MultiHeadAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                             num_heads=num_heads,\n",
    "                                             attn_dropout=attn_dropout)\n",
    "\n",
    "    self.mlp_block = MLPBlock(embedding_dim=embedding_dim,\n",
    "                              mlp_size=mlp_size,\n",
    "                              mlp_dropout=mlp_dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.msa_block(x) + x\n",
    "    x = self.mlp_block(x) + x\n",
    "    return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "  \"\"\"\n",
    "  Create Vision Transformer architecture model.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               img_size=IMG_SIZE, # Training resolution\n",
    "               in_channels=COLOR_CHANNELS, # Number of color channels in input image\n",
    "               patch_size=PATCH_SIZE, # Patch size\n",
    "               transformer_layer_num=TRANSFORMER_LAYER_NUM, # Number of ViT layers from ViT paper table\n",
    "               embedding_dim=EMBEDDING_DIM, # Hidden D size from ViT paper table\n",
    "               mlp_size=MLP_SIZE, # MLP size from ViT paper table\n",
    "               num_heads=NUM_HEADS, # Number of heads for MSA from ViT paper table\n",
    "               attn_dropout:float=0, # Dropout for attention from ViT paper table\n",
    "               mlp_dropout:float=0, # Dropout for MLP layers from ViT paper table\n",
    "               embedding_dropout=EMBEDDING_DROPOUT, # Dropout for patch and positional embedding\n",
    "               num_classes=CLASSES_NUM): # Number of classes to predict\n",
    "    super().__init__()\n",
    "\n",
    "    # Make sure the image size is divisible by the patch size\n",
    "    assert img_size % patch_size == 0, \"Image resolution must be divisible by the patch size\"\n",
    "\n",
    "    # Number of patches\n",
    "    self.num_patches = (img_size * img_size) // patch_size ** 2\n",
    "\n",
    "    # Create learnable class embedding\n",
    "    self.class_embedding = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "\n",
    "    # Create learnable positional embedding\n",
    "    self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embedding_dim))\n",
    "\n",
    "    # Dropout value for patch and positional embedding\n",
    "    self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "\n",
    "    # Create patch embedding layer\n",
    "    self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                          patch_size=patch_size,\n",
    "                                          embedding_dim=embedding_dim)\n",
    "\n",
    "    # Create Transformer blocks\n",
    "    self.transformer_layer = nn.Sequential(*[TransformerEncoder(embedding_dim=embedding_dim,\n",
    "                                                                num_heads=num_heads,\n",
    "                                                                mlp_size=mlp_size,\n",
    "                                                                attn_dropout=attn_dropout,\n",
    "                                                                mlp_dropout=mlp_dropout) for _ in range(transformer_layer_num)])\n",
    "\n",
    "    # Create classifier head\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "      nn.Linear(in_features=embedding_dim, out_features=num_classes))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Get batch size\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    # Create class token embeddding and expand it to the batch size\n",
    "    class_token = self.class_embedding.expand(batch_size, -1, -1)\n",
    "\n",
    "    # Apply patch embedding\n",
    "    x = self.patch_embedding(x)\n",
    "\n",
    "    # Concatenate class embedding and patch embedding\n",
    "    x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "    # Add positional embedding\n",
    "    x = x + self.positional_embedding\n",
    "\n",
    "    # Apply dropout to embedding part\n",
    "    x = self.embedding_dropout(x)\n",
    "\n",
    "    # Pass patch, class and positional embedding through the tranformer blocks\n",
    "    x = self.transformer_layer(x)\n",
    "\n",
    "    # 0 logit for classifier\n",
    "    x = self.classifier(x[:, 0])\n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b222f72",
   "metadata": {},
   "source": [
    "#### Making the model train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59179b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/engine.py\n",
    "\"\"\" \n",
    "File for training and testing our models.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def train_step(model: troch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               acc_fn: torchmetrics.Accuracy,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device)->Tuple[float, float]:\n",
    "    \"\"\" \n",
    "    Trains a PyTorch model for a single epoch.\n",
    "\n",
    "    Arguments:\n",
    "        - model: Pytorch model to be trained.\n",
    "        - dataloder: DataLoader for the model to be trained on.\n",
    "        - loss_fn: Pytorch criterion to minimize.\n",
    "        - acc_fn: Pytorch accuracy metric.\n",
    "        - optimizer: Optimize to help minimize the loss function.\n",
    "        - device: Target device to compute on.\n",
    "    \n",
    "    Returns:\n",
    "        - Tuple saving train training loss and train accuracy metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialization of train loss and accuracy\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X_train, y_train) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_train_pred = model(X_train)\n",
    "\n",
    "        # Calculate loss and accuracy\n",
    "        loss = loss_fn(y_train_pred, y_train)\n",
    "        train_loss += loss\n",
    "\n",
    "        accuracy = acc_fn(y_train_pred,y_train)\n",
    "        train_acc += accuracy\n",
    "        \n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "\n",
    "    return (train_loss, train_acc)\n",
    "\n",
    "def test_step(model: troch.nn.Module,\n",
    "            dataloader: torch.utils.data.DataLoader,\n",
    "            loss_fn: torch.nn.Module,\n",
    "            acc_fn: torchmetrics.Accuracy,\n",
    "            device: torch.device)->Tuple[float, float]:\n",
    "    \"\"\" \n",
    "    Tests a PyTorch model for a single epoch.\n",
    "\n",
    "    Arguments:\n",
    "        - model: Pytorch model to be tested.\n",
    "        - dataloder: DataLoader for the model to be tested on.\n",
    "        - loss_fn: Pytorch criterion to minimize.\n",
    "        - acc_fn: Pytorch accuracy metric.\n",
    "        - device: Target device to compute on.\n",
    "    \n",
    "    Returns:\n",
    "        - Tuple saving test loss and test accuracy metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialization of test loss and accuracy\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Disables gradient tracking to save memory and speed up computation during testing\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X_test, y_test) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_test_pred = model(X_test)\n",
    "\n",
    "            # Calculate loss and accuracy\n",
    "            loss = loss_fn(y_test_pred, y_test)\n",
    "            test_loss += loss\n",
    "\n",
    "            accuracy = acc_fn(y_test_pred, y_test)\n",
    "            test_acc += accuracy\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "\n",
    "    return (test_loss, test_acc)\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          acc_fn: torchmetrics.Accuracy,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          epochs: int,\n",
    "          device: torch.device)->Dict[str, list]:\n",
    "    \"\"\"\n",
    "    Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Argumentss:\n",
    "        - model: A PyTorch model to be trained and tested.\n",
    "        - train_dataloader: DataLoader for the model to be trained on.\n",
    "        - test_dataloader: DataLoader for the model to be tested on.\n",
    "        - loss_fn: Pytorch criterion to minimize.\n",
    "        - acc_fn: Pytorch accuracy metric.\n",
    "        - epochs: How many epochs to train for.\n",
    "        - device: Target device to compute on.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of training and testing loss as well as training and\n",
    "        testing accuracy metrics. Each metric has a value in a list for \n",
    "        each epoch.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           acc_fn=acc_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        acc_fn=acc_fn,\n",
    "                                        device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef8f01",
   "metadata": {},
   "source": [
    "### Making the utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd09d699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils.py\n",
    "\"\"\"\n",
    "File with utility functions for PyTorch model training and saving.\n",
    "\"\"\"\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "  \"\"\"\n",
    "  Saves a PyTorch model to a target directory.\n",
    "\n",
    "  Arguments:\n",
    "    - model: Target PyTorch model to save.\n",
    "    - target_dir: Directory for saving the model to. Default 'model'.\n",
    "    - model_name: Filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "  \"\"\"\n",
    "  # Create target directory\n",
    "  target_dir_path = Path(target_dir)\n",
    "  target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "\n",
    "  # Create model save path\n",
    "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "  model_save_path = target_dir_path / model_name\n",
    "\n",
    "  # Save the model state_dict()\n",
    "  print(f\"Saving model to: {model_save_path}\")\n",
    "  torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91313f",
   "metadata": {},
   "source": [
    "### Making the CNN train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4bd06728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_CNN.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_CNN.py\n",
    "\"\"\"\n",
    "Trains a PyTorch model using device-agnostic code.\n",
    "Can be controlled via command line arguments for hyperparameter tuning.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import torchmetrics\n",
    "from torchvision import transforms\n",
    "\n",
    "# Importing local modules\n",
    "from src import data_setup, engine, utils, CNN_builder\n",
    "\n",
    "def main():\n",
    "    # Setup ArgumentParser\n",
    "    parser = argparse.ArgumentParser(description=\"Train a Pytorch model on a choosen dataset.\")\n",
    "\n",
    "    # Add Arguments\n",
    "    parser.add_argument(\"--model_name\", \n",
    "                        type=str, \n",
    "                        default=\"CNN\", \n",
    "                        help=\"The filename for the saved model (e.g., 'CNN').\")\n",
    "\n",
    "    parser.add_argument(\"--train_dir\",\n",
    "                        type=str)\n",
    "\n",
    "    parser.add_argument(\"--test_dir\", \n",
    "                        type=str)\n",
    "    \n",
    "    parser.add_argument(\"--dataset\", \n",
    "                    type=str, \n",
    "                    default=None, \n",
    "                    help=\"Name of the dataset\")\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", \n",
    "                        type=int, \n",
    "                        default=32, \n",
    "                        help=\"Number of images per batch (default: 32).\")\n",
    "\n",
    "    parser.add_argument(\"--lr\", \n",
    "                        type=float, \n",
    "                        default=0.001, \n",
    "                        help=\"Learning rate for the optimizer (default: 0.001).\")\n",
    "\n",
    "    parser.add_argument(\"--num_epochs\", \n",
    "                        type=int, \n",
    "                        default=5, \n",
    "                        help=\"Number of training epochs (default: 5).\")\n",
    "\n",
    "    parser.add_argument(\"--hidden_units\", \n",
    "                        type=int, \n",
    "                        default=10, \n",
    "                        help=\"Number of hidden units in the neural network (default: 10).\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Setup target device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create transforms\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "        train_dir=args.train_dir,\n",
    "        test_dir=args.test_dir,\n",
    "        transform=data_transform,\n",
    "        batch_size=args.batch_size,\n",
    "        dataset_type=args.dataset\n",
    "    )\n",
    "\n",
    "    # Initialize the model\n",
    "    model = CNN_builder.CNN(\n",
    "        input_shape=3,\n",
    "        hidden_units=args.hidden_units,\n",
    "        output_shape=len(class_names)\n",
    "    ).to(device)\n",
    "\n",
    "    # Construct the path to the model file\n",
    "    model_save_path = os.path.join(\"models\", args.model_name if args.model_name.endswith(\".pth\") else args.model_name + \".pth\")\n",
    "    \n",
    "    if os.path.exists(model_save_path):\n",
    "        print(f\"Loading weights from: {model_save_path}\")\n",
    "        model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "    else:\n",
    "        print(f\"{model_save_path} not found. Training from scratch.\")\n",
    "\n",
    "    # Set loss, optimizer and accuracy function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    acc_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(class_names)).to(device)\n",
    "\n",
    "    # Start training\n",
    "    results = engine.train(model=model,\n",
    "                           train_dataloader=train_dataloader,\n",
    "                           test_dataloader=test_dataloader,\n",
    "                           loss_fn=loss_fn,\n",
    "                           optimizer=optimizer,\n",
    "                           acc_fn=acc_fn,\n",
    "                           epochs=args.num_epochs,\n",
    "                           device=device)\n",
    "\n",
    "    # Save the updated model\n",
    "    utils.save_model(model=model,\n",
    "                     target_dir=\"models\",\n",
    "                     model_name=f\"trained_{args.model_name}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301c987",
   "metadata": {},
   "source": [
    "### Making the ViT train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2fa62dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_ViT.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_ViT.py\n",
    "\"\"\"\n",
    "Trains a PyTorch Vision Transformer (ViT) model using device-agnostic code.\n",
    "Can be controlled via command line arguments for hyperparameter tuning.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import torchmetrics\n",
    "from torchvision import transforms\n",
    "\n",
    "# Importing local modules from the src/ directory\n",
    "from src import data_setup, engine, utils, ViT_builder\n",
    "\n",
    "def main():\n",
    "    # Setup ArgumentParser\n",
    "    parser = argparse.ArgumentParser(description=\"Train a Vision Transformer model on a chosen dataset.\")\n",
    "\n",
    "    # File & Path Arguments\n",
    "    parser.add_argument(\"--model_name\", \n",
    "                        type=str, \n",
    "                        default=\"ViT_Model\", \n",
    "                        help=\"The filename for the saved model.\")\n",
    "    \n",
    "    parser.add_argument(\"--dataset\", \n",
    "                    type=str, \n",
    "                    default=None, \n",
    "                    help=\"Name of the dataset\")\n",
    "\n",
    "    parser.add_argument(\"--train_dir\", \n",
    "                        type=str, \n",
    "                        default=\"data/pizza_steak_sushi/train\", \n",
    "                        help=\"Directory path for training data.\")\n",
    "\n",
    "    parser.add_argument(\"--test_dir\", \n",
    "                        type=str, \n",
    "                        default=\"data/pizza_steak_sushi/test\", \n",
    "                        help=\"Directory path for testing data.\")\n",
    "\n",
    "    # General Hyperparameters\n",
    "    parser.add_argument(\"--batch_size\", \n",
    "                        type=int, \n",
    "                        default=32, \n",
    "                        help=\"Number of images per batch.\")\n",
    "\n",
    "    parser.add_argument(\"--lr\", \n",
    "                        type=float, \n",
    "                        default=0.001, \n",
    "                        help=\"Learning rate for the optimizer.\")\n",
    "\n",
    "    parser.add_argument(\"--num_epochs\", \n",
    "                        type=int, \n",
    "                        default=5, \n",
    "                        help=\"Number of training epochs.\")\n",
    "\n",
    "    # ViT Specific Hyperparameters (Optimized for 64x64 by default)\n",
    "    parser.add_argument(\"--img_size\", \n",
    "                        type=int, \n",
    "                        default=64, \n",
    "                        help=\"Input image resolution.\")\n",
    "\n",
    "    parser.add_argument(\"--patch_size\", \n",
    "                        type=int, \n",
    "                        default=8, \n",
    "                        help=\"Patch size (must divide img_size).\")\n",
    "\n",
    "    parser.add_argument(\"--embedding_dim\", \n",
    "                        type=int, \n",
    "                        default=128, \n",
    "                        help=\"Hidden dimension size D.\")\n",
    "\n",
    "    parser.add_argument(\"--mlp_size\", \n",
    "                        type=int, \n",
    "                        default=512, \n",
    "                        help=\"MLP hidden size.\")\n",
    "\n",
    "    parser.add_argument(\"--num_heads\", \n",
    "                        type=int, \n",
    "                        default=8, \n",
    "                        help=\"Number of attention heads.\")\n",
    "\n",
    "    parser.add_argument(\"--num_layers\", \n",
    "                        type=int, \n",
    "                        default=8, \n",
    "                        help=\"Number of transformer encoder layers.\")\n",
    "\n",
    "    parser.add_argument(\"--dropout\", \n",
    "                        type=float, \n",
    "                        default=0.1, \n",
    "                        help=\"Dropout rate for attention and MLP.\")\n",
    "\n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Setup target device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create transforms (using the img_size argument)\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((args.img_size, args.img_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create DataLoaders with help from data_setup.py\n",
    "    train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "        train_dir=args.train_dir,\n",
    "        test_dir=args.test_dir,\n",
    "        transform=data_transform,\n",
    "        batch_size=args.batch_size,\n",
    "        dataset_type=args.dataset\n",
    "    )\n",
    "\n",
    "    # Initialize ViT model from ViT_builder.py\n",
    "    model = ViT_builder.ViT(\n",
    "        img_size=args.img_size,\n",
    "        in_channels=3,\n",
    "        patch_size=args.patch_size,\n",
    "        transformer_layer_num=args.num_layers,\n",
    "        embedding_dim=args.embedding_dim,\n",
    "        mlp_size=args.mlp_size,\n",
    "        num_heads=args.num_heads,\n",
    "        attn_dropout=args.dropout,\n",
    "        mlp_dropout=args.dropout,\n",
    "        embedding_dropout=args.dropout,\n",
    "        num_classes=len(class_names)\n",
    "    ).to(device)\n",
    "\n",
    "    # Set loss, optimizer and accuracy function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    acc_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(class_names)).to(device)\n",
    "\n",
    "    # Start training with help from engine.py\n",
    "    engine.train(model=model,\n",
    "                 train_dataloader=train_dataloader,\n",
    "                 test_dataloader=test_dataloader,\n",
    "                 loss_fn=loss_fn,\n",
    "                 optimizer=optimizer,\n",
    "                 acc_fn=acc_fn,\n",
    "                 epochs=args.num_epochs,\n",
    "                 device=device)\n",
    "\n",
    "    # Save the model with help from utils.py\n",
    "    MODEL_NAME = args.model_name if args.model_name.endswith(\".pth\") else args.model_name + \".pth\"\n",
    "    utils.save_model(model=model,\n",
    "                     target_dir=\"models\",\n",
    "                     model_name=MODEL_NAME)\n",
    "    print(f\"Model saved to: models/{MODEL_NAME}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
